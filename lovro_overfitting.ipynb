{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "lovro-overfitting.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ZvoneST/pytorch-labs/blob/master/lovro_overfitting.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ycCVB6FOEe__",
        "outputId": "145eb984-d554-4b95-b010-3c0b9493502e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.298716\n",
            "Train Epoch: 1 [640/60000 (1%)]\tLoss: 2.306512\n",
            "Train Epoch: 1 [1280/60000 (2%)]\tLoss: 2.293967\n",
            "Train Epoch: 1 [1920/60000 (3%)]\tLoss: 2.299939\n",
            "Train Epoch: 1 [2560/60000 (4%)]\tLoss: 2.303236\n",
            "Train Epoch: 1 [3200/60000 (5%)]\tLoss: 2.297542\n",
            "Train Epoch: 1 [3840/60000 (6%)]\tLoss: 2.303114\n",
            "Train Epoch: 1 [4480/60000 (7%)]\tLoss: 2.295347\n",
            "Train Epoch: 1 [5120/60000 (9%)]\tLoss: 2.296513\n",
            "Train Epoch: 1 [5760/60000 (10%)]\tLoss: 2.294319\n",
            "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 2.288235\n",
            "Train Epoch: 1 [7040/60000 (12%)]\tLoss: 2.278487\n",
            "Train Epoch: 1 [7680/60000 (13%)]\tLoss: 2.271936\n",
            "Train Epoch: 1 [8320/60000 (14%)]\tLoss: 2.269166\n",
            "Train Epoch: 1 [8960/60000 (15%)]\tLoss: 2.268812\n",
            "Train Epoch: 1 [9600/60000 (16%)]\tLoss: 2.246056\n",
            "Train Epoch: 1 [10240/60000 (17%)]\tLoss: 2.221994\n",
            "Train Epoch: 1 [10880/60000 (18%)]\tLoss: 2.181587\n",
            "Train Epoch: 1 [11520/60000 (19%)]\tLoss: 2.123480\n",
            "Train Epoch: 1 [12160/60000 (20%)]\tLoss: 1.992912\n",
            "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 1.728065\n",
            "Train Epoch: 1 [13440/60000 (22%)]\tLoss: 1.502969\n",
            "Train Epoch: 1 [14080/60000 (23%)]\tLoss: 0.966523\n",
            "Train Epoch: 1 [14720/60000 (25%)]\tLoss: 0.796153\n",
            "Train Epoch: 1 [15360/60000 (26%)]\tLoss: 0.796890\n",
            "Train Epoch: 1 [16000/60000 (27%)]\tLoss: 0.846418\n",
            "Train Epoch: 1 [16640/60000 (28%)]\tLoss: 0.604693\n",
            "Train Epoch: 1 [17280/60000 (29%)]\tLoss: 0.439424\n",
            "Train Epoch: 1 [17920/60000 (30%)]\tLoss: 0.822453\n",
            "Train Epoch: 1 [18560/60000 (31%)]\tLoss: 0.385367\n",
            "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.316659\n",
            "Train Epoch: 1 [19840/60000 (33%)]\tLoss: 0.389085\n",
            "Train Epoch: 1 [20480/60000 (34%)]\tLoss: 0.499878\n",
            "Train Epoch: 1 [21120/60000 (35%)]\tLoss: 0.377582\n",
            "Train Epoch: 1 [21760/60000 (36%)]\tLoss: 0.515374\n",
            "Train Epoch: 1 [22400/60000 (37%)]\tLoss: 0.250084\n",
            "Train Epoch: 1 [23040/60000 (38%)]\tLoss: 0.475571\n",
            "Train Epoch: 1 [23680/60000 (39%)]\tLoss: 0.177294\n",
            "Train Epoch: 1 [24320/60000 (41%)]\tLoss: 0.424517\n",
            "Train Epoch: 1 [24960/60000 (42%)]\tLoss: 0.431198\n",
            "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.532385\n",
            "Train Epoch: 1 [26240/60000 (44%)]\tLoss: 0.284960\n",
            "Train Epoch: 1 [26880/60000 (45%)]\tLoss: 0.340192\n",
            "Train Epoch: 1 [27520/60000 (46%)]\tLoss: 0.419784\n",
            "Train Epoch: 1 [28160/60000 (47%)]\tLoss: 0.231196\n",
            "Train Epoch: 1 [28800/60000 (48%)]\tLoss: 0.233612\n",
            "Train Epoch: 1 [29440/60000 (49%)]\tLoss: 0.175909\n",
            "Train Epoch: 1 [30080/60000 (50%)]\tLoss: 0.169502\n",
            "Train Epoch: 1 [30720/60000 (51%)]\tLoss: 0.317688\n",
            "Train Epoch: 1 [31360/60000 (52%)]\tLoss: 0.500306\n",
            "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.168516\n",
            "Train Epoch: 1 [32640/60000 (54%)]\tLoss: 0.285368\n",
            "Train Epoch: 1 [33280/60000 (55%)]\tLoss: 0.236115\n",
            "Train Epoch: 1 [33920/60000 (57%)]\tLoss: 0.258758\n",
            "Train Epoch: 1 [34560/60000 (58%)]\tLoss: 0.124842\n",
            "Train Epoch: 1 [35200/60000 (59%)]\tLoss: 0.298574\n",
            "Train Epoch: 1 [35840/60000 (60%)]\tLoss: 0.226623\n",
            "Train Epoch: 1 [36480/60000 (61%)]\tLoss: 0.341117\n",
            "Train Epoch: 1 [37120/60000 (62%)]\tLoss: 0.184695\n",
            "Train Epoch: 1 [37760/60000 (63%)]\tLoss: 0.207945\n",
            "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.502330\n",
            "Train Epoch: 1 [39040/60000 (65%)]\tLoss: 0.235725\n",
            "Train Epoch: 1 [39680/60000 (66%)]\tLoss: 0.304114\n",
            "Train Epoch: 1 [40320/60000 (67%)]\tLoss: 0.124988\n",
            "Train Epoch: 1 [40960/60000 (68%)]\tLoss: 0.174326\n",
            "Train Epoch: 1 [41600/60000 (69%)]\tLoss: 0.095132\n",
            "Train Epoch: 1 [42240/60000 (70%)]\tLoss: 0.165618\n",
            "Train Epoch: 1 [42880/60000 (71%)]\tLoss: 0.259281\n",
            "Train Epoch: 1 [43520/60000 (72%)]\tLoss: 0.314849\n",
            "Train Epoch: 1 [44160/60000 (74%)]\tLoss: 0.151823\n",
            "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.193044\n",
            "Train Epoch: 1 [45440/60000 (76%)]\tLoss: 0.125468\n",
            "Train Epoch: 1 [46080/60000 (77%)]\tLoss: 0.197806\n",
            "Train Epoch: 1 [46720/60000 (78%)]\tLoss: 0.176841\n",
            "Train Epoch: 1 [47360/60000 (79%)]\tLoss: 0.074175\n",
            "Train Epoch: 1 [48000/60000 (80%)]\tLoss: 0.066830\n",
            "Train Epoch: 1 [48640/60000 (81%)]\tLoss: 0.297903\n",
            "Train Epoch: 1 [49280/60000 (82%)]\tLoss: 0.355991\n",
            "Train Epoch: 1 [49920/60000 (83%)]\tLoss: 0.196745\n",
            "Train Epoch: 1 [50560/60000 (84%)]\tLoss: 0.122246\n",
            "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.271171\n",
            "Train Epoch: 1 [51840/60000 (86%)]\tLoss: 0.245624\n",
            "Train Epoch: 1 [52480/60000 (87%)]\tLoss: 0.176027\n",
            "Train Epoch: 1 [53120/60000 (88%)]\tLoss: 0.363249\n",
            "Train Epoch: 1 [53760/60000 (90%)]\tLoss: 0.307707\n",
            "Train Epoch: 1 [54400/60000 (91%)]\tLoss: 0.150088\n",
            "Train Epoch: 1 [55040/60000 (92%)]\tLoss: 0.133977\n",
            "Train Epoch: 1 [55680/60000 (93%)]\tLoss: 0.392306\n",
            "Train Epoch: 1 [56320/60000 (94%)]\tLoss: 0.206229\n",
            "Train Epoch: 1 [56960/60000 (95%)]\tLoss: 0.140572\n",
            "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.293192\n",
            "Train Epoch: 1 [58240/60000 (97%)]\tLoss: 0.174042\n",
            "Train Epoch: 1 [58880/60000 (98%)]\tLoss: 0.179303\n",
            "Train Epoch: 1 [59520/60000 (99%)]\tLoss: 0.088767\n",
            "\n",
            "Test set: Average loss: 1.0931, Accuracy: 7/11 (64%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:85: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
            "  warnings.warn(warning.format(ret))\n"
          ]
        }
      ],
      "source": [
        "from __future__ import print_function\n",
        "import argparse\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.autograd import Variable\n",
        "from torchvision import transforms as T\n",
        "import matplotlib.pyplot as plt\n",
        "from torchvision.datasets import ImageFolder\n",
        "# Training settings\n",
        "batch_size = 64\n",
        "\n",
        "# MNIST Dataset\n",
        "train_dataset = datasets.MNIST(root='./data/',\n",
        "                               train=True,\n",
        "                               transform=transforms.ToTensor(),\n",
        "                               download=True)\n",
        "data_transforms = transforms.Compose([\n",
        "    transforms.Grayscale(num_output_channels=1),transforms.ToTensor()])\n",
        "test_dataset = ImageFolder('lovro/', data_transforms)\n",
        "# Data Loader (Input Pipeline)\n",
        "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
        "                                           batch_size=batch_size,\n",
        "                                           shuffle=True)\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
        "                                          batch_size=batch_size,\n",
        "                                          shuffle=False)\n",
        "\n",
        "\n",
        "class Net(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
        "        #print(self.conv1.weight.shape)\n",
        "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
        "        self.conv3 = nn.Conv2d(20, 20, kernel_size=3)\n",
        "       #print(self.conv2.weight.shape)\n",
        "        self.mp = nn.MaxPool2d(2)\n",
        "        self.fc = nn.Linear(320, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        in_size = x.size(0)\n",
        "        x = F.relu(self.conv1(x))\n",
        "        #print(x.shape)\n",
        "        x = F.relu(self.mp(self.conv2(x)))\n",
        "        x = F.relu(self.mp(self.conv3(x)))\n",
        "        \n",
        "        #print(\"2.\", x.shape)\n",
        "       # x = F.relu(self.mp(self.conv3(x)))\n",
        "        x = x.view(in_size, -1)  # flatten the tensor\n",
        "        #print(\"3.\", x.shape)\n",
        "        x = self.fc(x)\n",
        "        return F.log_softmax(x)\n",
        "\n",
        "\n",
        "model = Net()\n",
        "\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.5)\n",
        "\n",
        "\n",
        "def train(epoch):\n",
        "    model.train()\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        data, target = Variable(data), Variable(target)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = F.nll_loss(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        if batch_idx % 10 == 0:\n",
        "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
        "                100. * batch_idx / len(train_loader), loss.item()))\n",
        "\n",
        "\n",
        "def test():\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    for data, target in test_loader:\n",
        "        data, target = Variable(data, volatile=True), Variable(target)\n",
        "        output = model(data)\n",
        "       # print(target, output)\n",
        "        # sum up batch loss\n",
        "        test_loss += F.nll_loss(output, target, size_average=False).data\n",
        "        # get the index of the max log-probability\n",
        "        pred = output.data.max(1, keepdim=True)[1]\n",
        "        correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
        "\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "        test_loss, correct, len(test_loader.dataset),\n",
        "        100. * correct / len(test_loader.dataset)))\n",
        "\n",
        "\n",
        "for epoch in range(1, 2):\n",
        "    train(epoch)\n",
        "    test()\n",
        "model_scripted = torch.jit.script(model) # Export to TorchScript\n",
        "model_scripted.save('model.pt') # Save"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_loader.dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qtSXATmGe03a",
        "outputId": "4fb561c4-a1c9-45c9-e002-ca01be494f1c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset ImageFolder\n",
              "    Number of datapoints: 11\n",
              "    Root location: lovro/\n",
              "    StandardTransform\n",
              "Transform: Compose(\n",
              "               Grayscale(num_output_channels=1)\n",
              "               ToTensor()\n",
              "           )"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf `find -type d -name .ipynb_checkpoints`"
      ],
      "metadata": {
        "id": "poIUBCNOFzhu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "imgs = torch.stack([img_t for img_t, _ in test_dataset], dim=1)\n",
        "imgs.view(1, -1).mean(dim=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QIWnWe89xgSw",
        "outputId": "18424226-dbfa-4b73-f97a-8acf9c3a4656"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0.0858])"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "imgs.view(1, -1).std(dim=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j7LMaIUOzWJ7",
        "outputId": "9186c958-e89e-42f0-936a-4cfcaad5d4ca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0.2801])"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "imgs = torch.stack([img_t for img_t, _ in train_dataset], dim=1)\n",
        "imgs.view(1, -1).mean(dim=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IZ1otSmu7Bzx",
        "outputId": "f984917f-bb1e-461d-f4da-429cf97c758d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0.1307])"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "imgs.view(1, -1).std(dim=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lt4JIIrc7GYo",
        "outputId": "5453bb33-5f5d-4808-b0b3-fa663fcb91e4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0.3081])"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "max_degree=30\n",
        "train_dataset = datasets.MNIST(root='./data/',\n",
        "                               train=True,\n",
        "                               transform=transforms.Compose([\n",
        "                     transforms.RandomAffine((0, 10)),\n",
        "                    transforms.RandomHorizontalFlip(),\n",
        "                        transforms.ToTensor(),\n",
        "                       # transforms.Normalize((0.1307,), (0.3081,))\n",
        "                    \n",
        "\n",
        "                                                            ]),\n",
        "                               download=False)\n",
        "data_transforms = transforms.Compose([\n",
        "    transforms.Grayscale(num_output_channels=1),transforms.ToTensor()])\n",
        "#test_dataset = ImageFolder('lovro/', data_transforms)\n",
        "# Data Loader (Input Pipeline)\n",
        "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
        "                                           batch_size=batch_size,\n",
        "                                           shuffle=True)\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
        "                                          batch_size=batch_size,\n",
        "                                          shuffle=False)"
      ],
      "metadata": {
        "id": "OkF5SVPL0JUo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "imgs, lbls = next(iter(train_loader))\n",
        "imgs[7].data.shape\n",
        "plt.imshow(imgs[7].data.reshape((28,28)), cmap=\"gray\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 284
        },
        "id": "eEg4cKjGwz61",
        "outputId": "62f08e39-0f07-4562-ea09-360e3e494690"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7fb52ea81590>"
            ]
          },
          "metadata": {},
          "execution_count": 41
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = Net()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.5)\n",
        "for epoch in range(1, 2):\n",
        "    train(epoch)\n",
        "    test()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c1LKbLLg0ZuM",
        "outputId": "9e69ca15-835c-461f-8910-8adddbc3c219"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.301778\n",
            "Train Epoch: 1 [640/60000 (1%)]\tLoss: 2.299991\n",
            "Train Epoch: 1 [1280/60000 (2%)]\tLoss: 2.301777\n",
            "Train Epoch: 1 [1920/60000 (3%)]\tLoss: 2.306104\n",
            "Train Epoch: 1 [2560/60000 (4%)]\tLoss: 2.299685\n",
            "Train Epoch: 1 [3200/60000 (5%)]\tLoss: 2.292202\n",
            "Train Epoch: 1 [3840/60000 (6%)]\tLoss: 2.281761\n",
            "Train Epoch: 1 [4480/60000 (7%)]\tLoss: 2.288778\n",
            "Train Epoch: 1 [5120/60000 (9%)]\tLoss: 2.276872\n",
            "Train Epoch: 1 [5760/60000 (10%)]\tLoss: 2.272185\n",
            "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 2.272032\n",
            "Train Epoch: 1 [7040/60000 (12%)]\tLoss: 2.272552\n",
            "Train Epoch: 1 [7680/60000 (13%)]\tLoss: 2.237378\n",
            "Train Epoch: 1 [8320/60000 (14%)]\tLoss: 2.202799\n",
            "Train Epoch: 1 [8960/60000 (15%)]\tLoss: 2.206467\n",
            "Train Epoch: 1 [9600/60000 (16%)]\tLoss: 2.107745\n",
            "Train Epoch: 1 [10240/60000 (17%)]\tLoss: 2.088117\n",
            "Train Epoch: 1 [10880/60000 (18%)]\tLoss: 1.863841\n",
            "Train Epoch: 1 [11520/60000 (19%)]\tLoss: 1.469635\n",
            "Train Epoch: 1 [12160/60000 (20%)]\tLoss: 1.191105\n",
            "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 1.018456\n",
            "Train Epoch: 1 [13440/60000 (22%)]\tLoss: 1.255480\n",
            "Train Epoch: 1 [14080/60000 (23%)]\tLoss: 0.905996\n",
            "Train Epoch: 1 [14720/60000 (25%)]\tLoss: 1.080773\n",
            "Train Epoch: 1 [15360/60000 (26%)]\tLoss: 0.921339\n",
            "Train Epoch: 1 [16000/60000 (27%)]\tLoss: 0.900930\n",
            "Train Epoch: 1 [16640/60000 (28%)]\tLoss: 1.139224\n",
            "Train Epoch: 1 [17280/60000 (29%)]\tLoss: 0.759637\n",
            "Train Epoch: 1 [17920/60000 (30%)]\tLoss: 0.653241\n",
            "Train Epoch: 1 [18560/60000 (31%)]\tLoss: 1.025210\n",
            "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 1.051811\n",
            "Train Epoch: 1 [19840/60000 (33%)]\tLoss: 0.586610\n",
            "Train Epoch: 1 [20480/60000 (34%)]\tLoss: 0.649126\n",
            "Train Epoch: 1 [21120/60000 (35%)]\tLoss: 0.493364\n",
            "Train Epoch: 1 [21760/60000 (36%)]\tLoss: 0.685278\n",
            "Train Epoch: 1 [22400/60000 (37%)]\tLoss: 0.601041\n",
            "Train Epoch: 1 [23040/60000 (38%)]\tLoss: 0.822006\n",
            "Train Epoch: 1 [23680/60000 (39%)]\tLoss: 0.749773\n",
            "Train Epoch: 1 [24320/60000 (41%)]\tLoss: 0.758569\n",
            "Train Epoch: 1 [24960/60000 (42%)]\tLoss: 0.561972\n",
            "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.537704\n",
            "Train Epoch: 1 [26240/60000 (44%)]\tLoss: 0.691034\n",
            "Train Epoch: 1 [26880/60000 (45%)]\tLoss: 0.638066\n",
            "Train Epoch: 1 [27520/60000 (46%)]\tLoss: 0.969518\n",
            "Train Epoch: 1 [28160/60000 (47%)]\tLoss: 0.740973\n",
            "Train Epoch: 1 [28800/60000 (48%)]\tLoss: 0.545953\n",
            "Train Epoch: 1 [29440/60000 (49%)]\tLoss: 0.927399\n",
            "Train Epoch: 1 [30080/60000 (50%)]\tLoss: 0.694956\n",
            "Train Epoch: 1 [30720/60000 (51%)]\tLoss: 0.605879\n",
            "Train Epoch: 1 [31360/60000 (52%)]\tLoss: 0.658306\n",
            "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.409134\n",
            "Train Epoch: 1 [32640/60000 (54%)]\tLoss: 0.640337\n",
            "Train Epoch: 1 [33280/60000 (55%)]\tLoss: 0.696200\n",
            "Train Epoch: 1 [33920/60000 (57%)]\tLoss: 0.526799\n",
            "Train Epoch: 1 [34560/60000 (58%)]\tLoss: 0.678939\n",
            "Train Epoch: 1 [35200/60000 (59%)]\tLoss: 0.528421\n",
            "Train Epoch: 1 [35840/60000 (60%)]\tLoss: 0.407972\n",
            "Train Epoch: 1 [36480/60000 (61%)]\tLoss: 0.438739\n",
            "Train Epoch: 1 [37120/60000 (62%)]\tLoss: 0.331346\n",
            "Train Epoch: 1 [37760/60000 (63%)]\tLoss: 0.512175\n",
            "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.532197\n",
            "Train Epoch: 1 [39040/60000 (65%)]\tLoss: 0.615304\n",
            "Train Epoch: 1 [39680/60000 (66%)]\tLoss: 0.549736\n",
            "Train Epoch: 1 [40320/60000 (67%)]\tLoss: 0.442099\n",
            "Train Epoch: 1 [40960/60000 (68%)]\tLoss: 0.724928\n",
            "Train Epoch: 1 [41600/60000 (69%)]\tLoss: 0.615563\n",
            "Train Epoch: 1 [42240/60000 (70%)]\tLoss: 0.497755\n",
            "Train Epoch: 1 [42880/60000 (71%)]\tLoss: 0.438650\n",
            "Train Epoch: 1 [43520/60000 (72%)]\tLoss: 0.421531\n",
            "Train Epoch: 1 [44160/60000 (74%)]\tLoss: 0.260740\n",
            "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.632221\n",
            "Train Epoch: 1 [45440/60000 (76%)]\tLoss: 0.521644\n",
            "Train Epoch: 1 [46080/60000 (77%)]\tLoss: 0.418111\n",
            "Train Epoch: 1 [46720/60000 (78%)]\tLoss: 0.370393\n",
            "Train Epoch: 1 [47360/60000 (79%)]\tLoss: 0.500767\n",
            "Train Epoch: 1 [48000/60000 (80%)]\tLoss: 0.349490\n",
            "Train Epoch: 1 [48640/60000 (81%)]\tLoss: 0.504406\n",
            "Train Epoch: 1 [49280/60000 (82%)]\tLoss: 0.416215\n",
            "Train Epoch: 1 [49920/60000 (83%)]\tLoss: 0.557291\n",
            "Train Epoch: 1 [50560/60000 (84%)]\tLoss: 0.420868\n",
            "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.550973\n",
            "Train Epoch: 1 [51840/60000 (86%)]\tLoss: 0.359464\n",
            "Train Epoch: 1 [52480/60000 (87%)]\tLoss: 0.515144\n",
            "Train Epoch: 1 [53120/60000 (88%)]\tLoss: 0.355811\n",
            "Train Epoch: 1 [53760/60000 (90%)]\tLoss: 0.270934\n",
            "Train Epoch: 1 [54400/60000 (91%)]\tLoss: 0.612718\n",
            "Train Epoch: 1 [55040/60000 (92%)]\tLoss: 0.278783\n",
            "Train Epoch: 1 [55680/60000 (93%)]\tLoss: 0.471629\n",
            "Train Epoch: 1 [56320/60000 (94%)]\tLoss: 0.624072\n",
            "Train Epoch: 1 [56960/60000 (95%)]\tLoss: 0.618845\n",
            "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.284583\n",
            "Train Epoch: 1 [58240/60000 (97%)]\tLoss: 0.397505\n",
            "Train Epoch: 1 [58880/60000 (98%)]\tLoss: 0.471052\n",
            "Train Epoch: 1 [59520/60000 (99%)]\tLoss: 0.419368\n",
            "\n",
            "Test set: Average loss: 1.7555, Accuracy: 6/11 (55%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:85: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
            "  warnings.warn(warning.format(ret))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Augmentation / Image transformation \n",
        "\n",
        "A deep network requires extensive data to achieve decent performance. To build a good classifier with small training data, image augmentation can solve the problem to a greater extend. Image augmentation generates images by different ways of processing, such as random shift, rotation, flips, etc.\n",
        "\n",
        "Below are the list of transformations that come pre-built with PyTorch:\n",
        "- ToTensor\n",
        "- ToPILImage\n",
        "- Normalize\n",
        "- Resize\n",
        "- Scale\n",
        "- CenterCrop\n",
        "- Pad\n",
        "- Lambda\n",
        "- RandomApply\n",
        "- RandomChoice\n",
        "- RandomOrder\n",
        "- RandomCrop\n",
        "- RandomHorizontalFlip\n",
        "- RandomVerticalFlip\n",
        "- RandomResizedCrop\n",
        "- RandomSizedCrop\n",
        "- FiveCrop\n",
        "- TenCrop\n",
        "- LinearTransformation\n",
        "- ColorJitter\n",
        "- RandomRotation\n",
        "- RandomAffine\n",
        "- Grayscale\n",
        "- RandomGrayscale\n",
        "- RandomPerspective\n",
        "- RandomErasing\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "JocDEp7719Xs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Ex 1 \n",
        "Consider which of the following data augmentation are useful in increasing the correct algorithm on the drawn examples. Implement the selected functions and check that you have increased the accuracy of the algorithm."
      ],
      "metadata": {
        "id": "354SdU9c1_tx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#your code"
      ],
      "metadata": {
        "id": "PVJqIS0Y3L9H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "_EjMV_Bd-QhZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dropout\n",
        "Yet another way to prevent overfitting is to build many models, then average their predictions at test time. Each model might have a different set of initial weights.\n",
        "\n",
        "We won't show an example of model averaging here. Instead, we will show another idea that sounds drastically different on the surface.\n",
        "\n",
        "This idea is called dropout: we will randomly \"drop out\", \"zero out\", or \"remove\" a portion of neurons from each training iteration.\n",
        "\n",
        "\n",
        "\n",
        "In different iterations of training, we will drop out a different set of neurons.\n",
        "\n",
        "The technique has an effect of preventing weights from being overly dependent on each other: for example for one weight to be unnecessarily large to compensate for another unnecessarily large weight with the opposite sign. Weights are encouraged to be \"more independent\" of one another.\n",
        "\n",
        "During test time though, we will not drop out any neurons; instead we will use the entire set of weights. This means that our training time and test time behaviour of dropout layers are different. In the code for the function train and get_accuracy, we use model.train() and model.eval() to flag whether we want the model's training behaviour, or test time behaviour.\n",
        "\n",
        "While unintuitive, using all connections is a form of model averaging! We are effectively averaging over many different networks of various connectivity structures."
      ],
      "metadata": {
        "id": "j1YCVYI8_fku"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# MNIST Dataset\n",
        "train_dataset = datasets.MNIST(root='./data/',\n",
        "                               train=True,\n",
        "                               transform=transforms.ToTensor(),\n",
        "                               download=True)\n",
        "data_transforms = transforms.Compose([\n",
        "    transforms.Grayscale(num_output_channels=1),transforms.ToTensor()])\n",
        "test_dataset = ImageFolder('lovro/', data_transforms)\n",
        "# Data Loader (Input Pipeline)\n",
        "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
        "                                           batch_size=batch_size,\n",
        "                                           shuffle=True)\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
        "                                          batch_size=batch_size,\n",
        "                                          shuffle=False)\n",
        "\n",
        "\n",
        "class Net(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
        "        nn.Dropout(0.2)\n",
        "        #print(self.conv1.weight.shape)\n",
        "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
        "        nn.Dropout(0.2)\n",
        "        self.conv3 = nn.Conv2d(20, 20, kernel_size=3)\n",
        "       #print(self.conv2.weight.shape)\n",
        "        nn.Dropout(0.2)\n",
        "        self.mp = nn.MaxPool2d(2)\n",
        "        self.fc = nn.Linear(320, 10)\n",
        "        nn.Dropout(0.2)\n",
        "    def forward(self, x):\n",
        "        in_size = x.size(0)\n",
        "        x = F.relu(self.conv1(x))\n",
        "        #print(x.shape)\n",
        "        x = F.relu(self.mp(self.conv2(x)))\n",
        "        x = F.relu(self.mp(self.conv3(x)))\n",
        "        \n",
        "        #print(\"2.\", x.shape)\n",
        "       # x = F.relu(self.mp(self.conv3(x)))\n",
        "        x = x.view(in_size, -1)  # flatten the tensor\n",
        "        #print(\"3.\", x.shape)\n",
        "        x = self.fc(x)\n",
        "        return F.log_softmax(x)\n",
        "\n",
        "\n",
        "model = Net()\n",
        "\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.5)"
      ],
      "metadata": {
        "id": "qqYcLVJJ-ZiL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(1, 2):\n",
        "    train(epoch)\n",
        "    test()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_KGIrURv-s0S",
        "outputId": "870ed114-0bd6-4732-a406-6867b036b981"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:46: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.306096\n",
            "Train Epoch: 1 [640/60000 (1%)]\tLoss: 2.303867\n",
            "Train Epoch: 1 [1280/60000 (2%)]\tLoss: 2.302498\n",
            "Train Epoch: 1 [1920/60000 (3%)]\tLoss: 2.296979\n",
            "Train Epoch: 1 [2560/60000 (4%)]\tLoss: 2.293898\n",
            "Train Epoch: 1 [3200/60000 (5%)]\tLoss: 2.296088\n",
            "Train Epoch: 1 [3840/60000 (6%)]\tLoss: 2.290628\n",
            "Train Epoch: 1 [4480/60000 (7%)]\tLoss: 2.290530\n",
            "Train Epoch: 1 [5120/60000 (9%)]\tLoss: 2.288451\n",
            "Train Epoch: 1 [5760/60000 (10%)]\tLoss: 2.279634\n",
            "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 2.286249\n",
            "Train Epoch: 1 [7040/60000 (12%)]\tLoss: 2.282610\n",
            "Train Epoch: 1 [7680/60000 (13%)]\tLoss: 2.268640\n",
            "Train Epoch: 1 [8320/60000 (14%)]\tLoss: 2.257897\n",
            "Train Epoch: 1 [8960/60000 (15%)]\tLoss: 2.252043\n",
            "Train Epoch: 1 [9600/60000 (16%)]\tLoss: 2.266900\n",
            "Train Epoch: 1 [10240/60000 (17%)]\tLoss: 2.225874\n",
            "Train Epoch: 1 [10880/60000 (18%)]\tLoss: 2.206649\n",
            "Train Epoch: 1 [11520/60000 (19%)]\tLoss: 2.126844\n",
            "Train Epoch: 1 [12160/60000 (20%)]\tLoss: 1.980650\n",
            "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 1.782073\n",
            "Train Epoch: 1 [13440/60000 (22%)]\tLoss: 1.144102\n",
            "Train Epoch: 1 [14080/60000 (23%)]\tLoss: 1.124821\n",
            "Train Epoch: 1 [14720/60000 (25%)]\tLoss: 1.009665\n",
            "Train Epoch: 1 [15360/60000 (26%)]\tLoss: 0.787421\n",
            "Train Epoch: 1 [16000/60000 (27%)]\tLoss: 0.777458\n",
            "Train Epoch: 1 [16640/60000 (28%)]\tLoss: 0.648057\n",
            "Train Epoch: 1 [17280/60000 (29%)]\tLoss: 0.691821\n",
            "Train Epoch: 1 [17920/60000 (30%)]\tLoss: 0.853413\n",
            "Train Epoch: 1 [18560/60000 (31%)]\tLoss: 0.440935\n",
            "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.551516\n",
            "Train Epoch: 1 [19840/60000 (33%)]\tLoss: 0.595912\n",
            "Train Epoch: 1 [20480/60000 (34%)]\tLoss: 0.637512\n",
            "Train Epoch: 1 [21120/60000 (35%)]\tLoss: 0.407744\n",
            "Train Epoch: 1 [21760/60000 (36%)]\tLoss: 0.379568\n",
            "Train Epoch: 1 [22400/60000 (37%)]\tLoss: 0.329931\n",
            "Train Epoch: 1 [23040/60000 (38%)]\tLoss: 0.259670\n",
            "Train Epoch: 1 [23680/60000 (39%)]\tLoss: 0.529322\n",
            "Train Epoch: 1 [24320/60000 (41%)]\tLoss: 0.385017\n",
            "Train Epoch: 1 [24960/60000 (42%)]\tLoss: 0.577017\n",
            "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.549182\n",
            "Train Epoch: 1 [26240/60000 (44%)]\tLoss: 0.281520\n",
            "Train Epoch: 1 [26880/60000 (45%)]\tLoss: 0.253385\n",
            "Train Epoch: 1 [27520/60000 (46%)]\tLoss: 0.368635\n",
            "Train Epoch: 1 [28160/60000 (47%)]\tLoss: 0.557284\n",
            "Train Epoch: 1 [28800/60000 (48%)]\tLoss: 0.340311\n",
            "Train Epoch: 1 [29440/60000 (49%)]\tLoss: 0.208969\n",
            "Train Epoch: 1 [30080/60000 (50%)]\tLoss: 0.362293\n",
            "Train Epoch: 1 [30720/60000 (51%)]\tLoss: 0.215354\n",
            "Train Epoch: 1 [31360/60000 (52%)]\tLoss: 0.211547\n",
            "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.206124\n",
            "Train Epoch: 1 [32640/60000 (54%)]\tLoss: 0.156249\n",
            "Train Epoch: 1 [33280/60000 (55%)]\tLoss: 0.158304\n",
            "Train Epoch: 1 [33920/60000 (57%)]\tLoss: 0.262830\n",
            "Train Epoch: 1 [34560/60000 (58%)]\tLoss: 0.224738\n",
            "Train Epoch: 1 [35200/60000 (59%)]\tLoss: 0.353334\n",
            "Train Epoch: 1 [35840/60000 (60%)]\tLoss: 0.358065\n",
            "Train Epoch: 1 [36480/60000 (61%)]\tLoss: 0.275213\n",
            "Train Epoch: 1 [37120/60000 (62%)]\tLoss: 0.196470\n",
            "Train Epoch: 1 [37760/60000 (63%)]\tLoss: 0.396821\n",
            "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.334570\n",
            "Train Epoch: 1 [39040/60000 (65%)]\tLoss: 0.161863\n",
            "Train Epoch: 1 [39680/60000 (66%)]\tLoss: 0.151350\n",
            "Train Epoch: 1 [40320/60000 (67%)]\tLoss: 0.201906\n",
            "Train Epoch: 1 [40960/60000 (68%)]\tLoss: 0.569934\n",
            "Train Epoch: 1 [41600/60000 (69%)]\tLoss: 0.356275\n",
            "Train Epoch: 1 [42240/60000 (70%)]\tLoss: 0.176795\n",
            "Train Epoch: 1 [42880/60000 (71%)]\tLoss: 0.196215\n",
            "Train Epoch: 1 [43520/60000 (72%)]\tLoss: 0.191359\n",
            "Train Epoch: 1 [44160/60000 (74%)]\tLoss: 0.281166\n",
            "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.215556\n",
            "Train Epoch: 1 [45440/60000 (76%)]\tLoss: 0.188422\n",
            "Train Epoch: 1 [46080/60000 (77%)]\tLoss: 0.454219\n",
            "Train Epoch: 1 [46720/60000 (78%)]\tLoss: 0.212527\n",
            "Train Epoch: 1 [47360/60000 (79%)]\tLoss: 0.256599\n",
            "Train Epoch: 1 [48000/60000 (80%)]\tLoss: 0.087749\n",
            "Train Epoch: 1 [48640/60000 (81%)]\tLoss: 0.382023\n",
            "Train Epoch: 1 [49280/60000 (82%)]\tLoss: 0.192203\n",
            "Train Epoch: 1 [49920/60000 (83%)]\tLoss: 0.208308\n",
            "Train Epoch: 1 [50560/60000 (84%)]\tLoss: 0.151077\n",
            "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.067118\n",
            "Train Epoch: 1 [51840/60000 (86%)]\tLoss: 0.227257\n",
            "Train Epoch: 1 [52480/60000 (87%)]\tLoss: 0.145117\n",
            "Train Epoch: 1 [53120/60000 (88%)]\tLoss: 0.202191\n",
            "Train Epoch: 1 [53760/60000 (90%)]\tLoss: 0.127403\n",
            "Train Epoch: 1 [54400/60000 (91%)]\tLoss: 0.104797\n",
            "Train Epoch: 1 [55040/60000 (92%)]\tLoss: 0.123735\n",
            "Train Epoch: 1 [55680/60000 (93%)]\tLoss: 0.149851\n",
            "Train Epoch: 1 [56320/60000 (94%)]\tLoss: 0.113309\n",
            "Train Epoch: 1 [56960/60000 (95%)]\tLoss: 0.134362\n",
            "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.129359\n",
            "Train Epoch: 1 [58240/60000 (97%)]\tLoss: 0.152759\n",
            "Train Epoch: 1 [58880/60000 (98%)]\tLoss: 0.195392\n",
            "Train Epoch: 1 [59520/60000 (99%)]\tLoss: 0.097363\n",
            "\n",
            "Test set: Average loss: 1.3436, Accuracy: 8/11 (73%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:85: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
            "  warnings.warn(warning.format(ret))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ex2  \n",
        "Try to combine data augmentation and dropout idea and compare result of convolutional neural network."
      ],
      "metadata": {
        "id": "CgJSntNaAwJI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#your code"
      ],
      "metadata": {
        "id": "OgSSpaVSBgax"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Weight Decay\n",
        "A more interesting technique that prevents overfitting is the idea of weight decay. The idea is to **penalize large weights**. We avoid large weights, because large weights mean that the prediction relies a lot on the content of one pixel, or on one unit. Intuitively, it does not make sense that the classification of an image should depend heavily on the content of one pixel, or even a few pixels.\n",
        "\n",
        "Mathematically, we penalize large weights by adding an extra term to the loss function, the term can look like the following:<br>\n",
        "\n",
        "*  $L^1$ regularization $\\sum_{k} |w_k|$<br>\n",
        "Mathematically, this term encourages weights to be exactly 0.<br><br>\n",
        "*  $L^2$ regularization $\\sum_{k} w_k^2$<br>\n",
        "Mathematically, in each iteration the weight is pushed towards 0.<br><br>\n",
        "*  Combination of $L^1$ and $L^2$ regularization: add a term  $\\sum_{k} |w_k| + w_k^2$ to the loss function.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "In PyTorch, weight decay can also be done automatically inside an optimizer. The parameter weight_decay of optim.SGD and most other optimizers uses  L2  regularization for weight decay. The value of the weight_decay parameter is another tunable hyperparameter."
      ],
      "metadata": {
        "id": "lizmmwXCBiMN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.5, weight_decay=1e-5)"
      ],
      "metadata": {
        "id": "avD6AFsRGFHV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ex3 \n",
        "Try which combination of methods (or maybe all) makes the best possible result."
      ],
      "metadata": {
        "id": "MA9rzKInJ-wT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#your code"
      ],
      "metadata": {
        "id": "1zjt2H6dKgFt"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}